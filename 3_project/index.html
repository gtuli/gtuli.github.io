<!DOCTYPE html>
<html lang='en'>
<head><meta charset='UTF-8'><title>Project</title></head>
<body>
<h1>3 Project</h1>
<img src='placeholder.jpg' alt='Project Graphic' style='width:300px;height:auto;'>
<h2>Key Questions Addressed</h2>
<ul>
<li>Feasibility of Gaze as a Proxy: Can an infant’s gaze direction and duration serve as a reliable proxy for saying “I see that”? Vision Ninja sought to determine if gaze tracking technology is sufficiently accurate and discriminative for measuring visual acuity (e.g., does the baby look at the target when they see it, and look away when it’s too small to see?).</li>
<li>Optimal Test Strategy: What is the best sequence and timing to present stimuli to get a valid acuity measurement quickly? The project explored parameters like how long to show each optotype, how to change size (up or down) based on the child’s responses, and when to repeat sizes for confirmation. Essentially, it addresses how to adaptively find the threshold of vision.</li>
<li>Attention and Engagement: How to keep a young child engaged enough to get through the test? Infants have short attention spans. The project had to figure out design elements (sounds, animations, etc.) to attract the child’s gaze sufficiently for the test. It also considered if a brief training** or calibration is needed (for example, getting the baby used to looking at the tablet initially).</li>
</ul>
<h2>The Problem</h2>
<ul>
<li>Inefficient Current Exams: Presently, pediatric ophthalmologists use methods like fix-and-follow, or Teller acuity cards, which require manual observation and often multiple clinic visits to gauge an infant’s vision. These can take a long time and yield only a rough estimate of acuity. The manual nature also introduces observer bias and inconsistency.</li>
<li>Late Detection of Issues: Because infant eye exams are so challenging, subtle visual deficits (e.g., mild amblyopia or refractive error) might go undetected until the child is older and can cooperate with standard tests. This delay can miss a critical window when interventions (patching, corrective lenses) would be most effective.</li>
<li>Stress and Cooperation: For parents and babies, the exam process can be stressful – keeping a toddler’s focus, dealing with tantrums – which further limits exam accuracy. A long, tedious exam often results in an upset child who won’t cooperate by the end, leading to incomplete or unreliable results. There was a need for a shorter, engaging test that could yield reliable data before a child loses interest.</li>
</ul>
<h2>The Importance</h2>
<ul>
<li>Early Intervention: Detecting vision problems in the first 2 years of life can significantly improve outcomes (e.g., preventing lifelong vision impairment from lazy eye). A reliable infant acuity test could become a standard screening, ensuring issues like severe farsightedness or astigmatism are caught and corrected early.</li>
<li>Reducing Exam Time: A faster test (30% quicker as achieved) means less distress for the child and family, and more toddlers can be tested in a clinic session. This not only improves patient experience but also allows ophthalmology clinics to serve more patients (addressing wait times for pediatric eye appointments).</li>
<li>Objective Measurement: Vision Ninja provides quantitative metrics (like exact gaze reaction times and threshold optotype size) as opposed to subjective judgment of an infant’s behavior. Objective data can be tracked over time to monitor improvement or worsening of vision – useful for evaluating treatments (e.g., cataract surgery follow-up in babies, or progress with glasses).</li>
<li>Scalable to Primary Care: If simplified and validated, such a tool could even be used in pediatricians’ offices for screening, not just specialist clinics. This wide reach would ensure more children get early vision screening, analogous to how hearing tests are done for newborns.</li>
</ul>
<h2>The Solution</h2>
<ul>
<li>Adaptive Optotype Display: The Vision Ninja app displays optotypes (high-contrast shapes akin to those on eye charts) one at a time on an iPad screen. The shape’s size changes based on the infant’s responses: it decreases in size after a correct response (the child looked at it) and increases if the child fails to look, following a staircase strategy. This continues until the smallest discernible size is found – effectively determining the child’s visual acuity threshold.</li>
<li>Gaze Tracking via OpenCV: Using the tablet’s front camera, the app employs computer vision algorithms (OpenCV) to detect the baby’s face and eyes. It determines if the infant’s gaze is directed at the optotype. A “meaningful gaze” was defined (e.g., child fixates on the target within 1 second and maintains gaze ~2 seconds). If such a gaze is detected, the app counts it as the child seeing the optotype; if not, it’s counted as not seen.</li>
<li>Automated Test Flow: The testing flows with minimal manual intervention:</li>
<li>The examiner enters basic info (patient ID, age) into the app.</li>
<li>The test begins at a mid-level optotype size (analogous to 20/40 on Snellen, for example).</li>
<li>If the child looks at the shape correctly (within the predefined time), the next shape shown is one size smaller. If the child does not respond, the next shape is larger.</li>
<li>The location of the shape on screen is randomized each time to ensure the child truly has to find it visually, not just stare at one spot.</li>
<li>This adaptively continues, “homing in” on the threshold. Once the app finds a size where the child cannot identify the optotype in 2–3 attempts, and the next larger size they can (confirming that boundary), the test ends.</li>
<li>Data and Scoring: For each optotype presented, the app records:</li>
<li>Size of the optotype,</li>
<li>Whether it was “identified” (meaningful gaze detected) and how quickly,</li>
<li>Gaze maintenance time (did the child continue looking for a sustained period). These metrics feed into a “Score” for that test, summarizing performance. The threshold smallest size seen is translated to a standard acuity measure (e.g., 20/80 vision).</li>
<li>Calibration and Distance: The prototype assumes a fixed testing distance (the child ~40 cm from the device, which is about 16 inches) to mimic the standardized distance used in vision charts. This was facilitated by using a holder or the parent holding the iPad at roughly that distance. (Future versions might use the camera to measure distance or use a device mount.) Also, initial calibration ensures the app can detect the child’s eyes properly (adjusting for lighting, etc.), although in the prototype this was a quick manual check rather than a formal step.</li>
</ul>
<h2>Architecture Overview</h2>
<ul>
<li>Mobile Application Structure: Vision Ninja’s app is structured with a simple UI: a start screen for inputting patient info, the main test module, and an optional clinician dashboard.</li>
<li>The Main Test Module runs a loop controlling stimulus presentation and calling the gaze tracking function. It was implemented in Swift (for iOS) with embedded Python/OpenCV for the eye-tracking analysis (using Swift-Python interoperability).</li>
<li>The Gaze Tracking Engine (OpenCV) processes each camera frame to find facial landmarks (eyes). It uses filters to reduce noise (ensuring it reacts only to deliberate eye movements, not random glances). The logic determines if the child’s gaze intersects the region of the screen where the optotype is displayed, within the allowed response time.</li>
<li>State Machine for Adaptive Testing: The app maintains a state of current optotype size and logic for increasing/decreasing size. This functions like a state machine or decision tree: if “seen” → go to smaller state, if “not seen” → go to larger state. It also ensures boundary conditions: if the child can’t see even the largest size or can see the smallest, it ends accordingly.</li>
<li>Data Storage: All test results are saved locally (and can be exported as a report). Each optotype trial’s data (size, response time, success/fail) is logged. This could later be integrated with a hospital EHR or cloud for aggregated analysis.</li>
<li>Prototype Dashboard (Future Scope): Although not fully implemented in the initial prototype, a concept for a clinician dashboard was outlined. It would allow review of the results (like a summary “score” indicating acuity category pass/fail, and perhaps a trend if the child has multiple tests over time). It might also have controls to adjust test parameters or to replay what the baby saw (for training or parent education).</li>
<li>Adaptive Algorithm Details: The algorithm for stepping sizes is based on a standard psychometric testing approach (staircase method). For example, it might decrease size by 1 step after each success and increase by 1 after each failure, and near the threshold ensure a few reversals to confirm the true threshold. In the Vision Ninja notes, they specified repeating the threshold size at least 3 times to confirm meaningful gaze – this is built into the test flow logic.</li>
<li>Attention Safeguards: To handle moments when a child is distracted, the app can pause and play an attention-getter (like a sound or animation) to recapture the child’s gaze before resuming. This ensures that a “miss” is truly because the optotype was too small, not because the child wasn’t looking at the screen. The prototype allowed manual pausing by the examiner if needed and had a feature to display a fun image to reset the child’s attention.</li>
<li>Result Determination: At test completion, the smallest optotype size the child consistently identified (and/or the last few successes and fails) are used to assign an acuity measure. For instance, if size “3” (which might correspond to 20/40) was the smallest seen, the child’s acuity is around 20/40. If they saw down to size “1” (20/20 equivalent), it’s normal, etc. This mapping was predetermined by calibrating the on-screen sizes to Snellen or LogMAR standards (the Vision Ninja notes mention correlating optotype sizes to Snellen acuity at 40 cm distance).</li>
</ul>
<h2>Results and Impacts</h2>
<ul>
<li>Dramatic Reduction in Exam Time: In pilot tests with a small group of toddlers, Vision Ninja achieved about a 30% reduction in exam duration compared to conventional visual acuity tests for that age group. What typically took ~10 minutes of coaxing and observing could be done in ~7 minutes with the interactive app. This time savings reduces stress on both clinicians and the child, and increases throughput in busy clinics.</li>
<li>Objective, Quantifiable Outcomes: The tool provided hard data – for each test, a record of reaction times and a quantified acuity level. This is a leap from the largely subjective notes (“child tracks toy – maybe 20/80 vision”) to an objective report (“child’s visual acuity ~20/80, based on gaze responses, with X% confidence”). Over multiple visits, these metrics allow tracking of a child’s visual development or the effect of interventions (e.g., measuring improvement after starting to wear glasses).</li>
<li>Successful Gaze Detection: The prototype demonstrated that even with basic algorithms, the iPad camera could reliably detect whether a child was looking at the target. In testing, gaze tracking accuracy was high for clear cases (children looking directly at the stimulus). There were instances of indeterminate gaze, but the algorithm’s requirement for sustained look helped filter out false positives. This success validated the feasibility of nonverbal gaze-based acuity measurement.</li>
<li>Engagement and Ease of Use: The app’s interactive nature (shapes appearing with some movement or sound) kept many toddlers engaged enough to complete the test. Parents and clinicians gave positive feedback on the child being more “cooperative” since it felt like a game rather than a medical exam. This is a qualitative but important impact – turning a challenging exam into a playful activity.</li>
<li>Clinical Potential: Ophthalmologists who saw the demo expressed that this could fill a critical gap in their toolkit. A refined version could become a standard part of well-child visits or pediatric ophthalmology exams. The project thus has sparked interest in further development, possibly into a product for broader use. Its impact is also in concept: it showed that using modern computer vision techniques in everyday clinical exams (not just high-end labs) is achievable.</li>
</ul>
<h2>Skills and Tools Used</h2>
<ul>
<li>Computer Vision (OpenCV): Implemented real-time eye/gaze tracking. Dr. Tuli utilized OpenCV’s face and eye detection capabilities and customized them for infant faces (which required tweaking parameters, as infant facial proportions differ from adults). He also applied image processing to improve detection (e.g., converting to grayscale, using filters to reduce noise from movements).</li>
<li>Mobile App Development: Built the prototype app on iOS. This involved Swift programming for the UI and integrating with the device camera APIs. He leveraged Apple’s AVFoundation for camera capture and employed a Swift-Python bridge or native OpenCV for iOS to run the tracking.</li>
<li>UX Design for Toddlers: Though not a typical “skill” in engineering terms, designing an interface for babies required creativity – using high-contrast, simple shapes (since infants respond well to those), and ensuring quick feedback. This also meant understanding and implementing timing logic to coincide with a baby’s behaviors (for example, showing stimuli long enough for a baby’s slower reaction).</li>
<li>Data Logging and Analysis: Built in logging to record each optotype presentation and outcome. After sessions, these logs were analyzed (using Python/pandas) to refine the algorithm thresholds (e.g., was 1 second too short for initial gaze? Did we need to extend maintain time beyond 2 seconds for certain ages?). This iterative analysis improved the test parameters.</li>
<li>Testing and Iteration: Conducted hands-on testing with actual users (infants). This experience required adjusting to on-the-fly challenges, a skill in usability testing. He applied an agile approach: test with a few patients, gather feedback from clinicians and parents, tweak the app, and repeat.</li>
<li>Communication with Clinicians: Worked closely with pediatric ophthalmologists and optometrists to validate the approach. He translated technical capabilities into clinical terms – for instance, explaining how “meaningful gaze” correlates to a child recognizing an object. He also needed to align the test outcomes with what clinicians expect (like correlating it to “approximate Snellen acuity” so they trust and understand it). This interdisciplinary communication skill ensured the tool met clinical needs.</li>
</ul>
<h2>Cross-project Capabilities</h2>
<ul>
<li>Human-Centered AI Design: Vision Ninja exemplifies designing an AI-driven tool around the end-user’s (in this case, an infant patient and the clinician) experience. Dr. Tuli carries this user-centered philosophy to other projects – whether designing dashboards for clinicians (as in the Mother-Infant Dashboard) or AI tools that fit naturally into workflows.</li>
<li>Real-time Signal Processing: The rapid video processing and decision-making (within seconds) in Vision Ninja parallels real-time data processing in Pandemic Pulse (processing streaming data) and real-time monitoring in the ICU project. It highlights Dr. Tuli’s ability to develop solutions that operate in real-time environments with quick feedback loops.</li>
<li>Integration of Hardware and Software: By working on a project that involves device hardware (camera, tablet sensors) and software algorithms, Dr. Tuli furthered skills in embedded systems and edge AI. This capability is useful in IoT-like healthcare solutions (similar to how, in the ICU project, he deals with medical device data streams, albeit not directly programming the devices).</li>
<li>Innovating in Constrained Conditions: Vision Ninja had to work with limited data (a few minutes of a baby’s attention) and with non-verbal cues. Innovating under such constraints – extracting meaningful information when data is scarce or noisy – is a skill that he has applied across projects, such as detecting weak biosignals in Hidden Signal or mapping sparse data in the mother-infant context.</li>
<li>Demonstration and Advocacy: This project, being quite visual and novel, allowed Dr. Tuli to demonstrate a clear “wow factor” prototype to stakeholders. The ability to create convincing prototypes that illustrate the power of AI is a capability he’s leveraged to secure buy-in and funding in various initiatives. For example, building a quick demo for a mother-infant dashboard using synthetic data (Project 6) similarly helped stakeholders visualize the end goal.</li>
</ul>
<h2>Published Papers/Tools</h2>
<ul>
<li>Prototype Documentation: “Vision Ninja – Validation Prototype Definition Notes (Oct 19 2022)” – an internal document detailing the design and initial results. It served as a reference for the project team and for discussions with the hospital’s digital health accelerator.</li>
<li>Conference Abstract (In Preparation): The team has prepared an abstract titled “Gaze-Tracking for Infant Visual Acuity: Feasibility Study” intended for submission to a conference like ARVO (Association for Research in Vision and Ophthalmology). This would formally present the methodology and initial results to the scientific community.</li>
<li>Software Tool (Prototype): The Vision Ninja app (prototype) is an installable iPad application (not yet publicly available). A video demonstration of the app in use was created and shared internally with the hospital’s Innovation & Digital Health Accelerator, illustrating how the child interacts and how gaze is detected.</li>
<li>Related Award: This project concept contributed to Dr. Tuli’s team winning a small innovation grant at Boston Children’s (the “FastTrack Innovation in Pediatric Healthcare” award 2022). While not a publication, it is a recognition of the work’s impact and was accompanied by a poster and demo session.</li>
</ul>
<p><a href='../index.html'>&larr; Back to Projects</a></p>
</body></html>